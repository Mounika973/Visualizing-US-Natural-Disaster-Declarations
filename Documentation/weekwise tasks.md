
**Week 1 – Data Understanding, Loading and Cleaning**
In the first week, we worked with a standard dataset and learned how to load, transform, and visualize data using Power BI. As part of the U.S. natural disaster visualization project, we worked with datasets from FEMA. We cleaned the data by replacing incorrect values, removing null values, and deleting unwanted columns. This week helped in understanding the importance of clean and structured data.



**Week 2 – Data Cleaning and Exploratory Data Analysis Using Python**
In the second week, we learned how to clean and analyze data using Python in Jupyter Notebook. We applied data preprocessing techniques programmatically and performed exploratory data analysis to understand data patterns, trends, and insights. We practiced EDA using the Netflix dataset, which helped in understanding real-world datasets before visualization.



**Week 3 – Live Data Concepts and API Basics**
In the third week, we were introduced to the concept of live data and how real-time data works. We discussed different ways of fetching live data and understood how APIs act as an interface between web data servers and Power BI. This week focused on understanding how dynamic data is accessed and refreshed in dashboards.



**Week 4 – Live Data Integration, CRM, and ETL Pipeline**
In the fourth week, we worked on understanding live data in detail, specifically how to connect live data sources using web or API. We learned how live data connections function and how such data can be directly integrated into Power BI dashboards.

Along with this, we discussed Customer Relationship Management concepts and explored Zendesk, which is a cloud-based customer service platform mainly used for ticketing and customer support management.

We also learned about the ETL pipeline, which stands for Extract, Transform, and Load, and understood its role in real-world data analytics workflows.


